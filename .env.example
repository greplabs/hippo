# Hippo Web Server Configuration
# Copy this file to .env and customize as needed

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================

# Server host (use 0.0.0.0 to accept connections from any network interface)
HIPPO_HOST=0.0.0.0

# Server port
HIPPO_PORT=3000

# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================

# Path to SQLite database (inside container)
HIPPO_DB_PATH=/data/hippo.db

# =============================================================================
# QDRANT VECTOR DATABASE (OPTIONAL)
# =============================================================================

# Qdrant connection URL (use http://qdrant:6334 for Docker Compose)
QDRANT_URL=http://qdrant:6334

# Enable/disable Qdrant features (true/false)
QDRANT_ENABLED=true

# Qdrant HTTP API port (exposed to host)
QDRANT_PORT=6333

# Qdrant gRPC API port (exposed to host)
QDRANT_GRPC_PORT=6334

# Qdrant logging level (DEBUG, INFO, WARN, ERROR)
QDRANT_LOG_LEVEL=INFO

# =============================================================================
# FILE INDEXING PATHS
# =============================================================================

# Paths to index (mount these as read-only volumes)
# Customize these to point to your actual directories

HIPPO_INDEX_PATH_1=./storage
HIPPO_INDEX_PATH_2=./snapshots
# HIPPO_INDEX_PATH_3=/path/to/your/documents
# HIPPO_INDEX_PATH_4=/path/to/your/photos
# HIPPO_INDEX_PATH_5=/path/to/your/videos

# =============================================================================
# AI FEATURES (OPTIONAL)
# =============================================================================

# Ollama host (for local AI features)
# Use host.docker.internal to connect to Ollama running on host machine
OLLAMA_HOST=http://host.docker.internal:11434

# Default AI model for chat
OLLAMA_CHAT_MODEL=qwen2:0.5b

# Default AI model for embeddings
OLLAMA_EMBED_MODEL=nomic-embed-text

# =============================================================================
# LOGGING & DEBUGGING
# =============================================================================

# Rust log level (error, warn, info, debug, trace)
# Format: target=level,target=level (e.g., hippo_web=debug,tower_http=info)
RUST_LOG=info

# Enable backtraces (0=disabled, 1=enabled, full=full backtrace)
RUST_BACKTRACE=0

# =============================================================================
# PERFORMANCE TUNING
# =============================================================================

# Number of Tokio worker threads (default: number of CPU cores)
TOKIO_WORKER_THREADS=4

# =============================================================================
# RESOURCE LIMITS (DOCKER COMPOSE)
# =============================================================================

# Hippo Web Service Limits
HIPPO_CPU_LIMIT=2.0
HIPPO_MEM_LIMIT=2G
HIPPO_CPU_RESERVE=0.5
HIPPO_MEM_RESERVE=512M

# Qdrant Service Limits
QDRANT_CPU_LIMIT=1.0
QDRANT_MEM_LIMIT=1G
QDRANT_CPU_RESERVE=0.25
QDRANT_MEM_RESERVE=256M

# =============================================================================
# DEVELOPMENT MODE (OPTIONAL)
# =============================================================================

# Enable development features
# HIPPO_DEV_MODE=true

# Development data directory
# HIPPO_DEV_DATA_DIR=./dev-data
