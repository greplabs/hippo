<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="Ollama integration for local AI capabilities"><title>hippo_core::ollama - Rust</title><script>if(window.location.protocol!=="file:")document.head.insertAdjacentHTML("beforeend","SourceSerif4-Regular-6b053e98.ttf.woff2,FiraSans-Italic-81dc35de.woff2,FiraSans-Regular-0fe48ade.woff2,FiraSans-MediumItalic-ccf7e434.woff2,FiraSans-Medium-e1aa3f0a.woff2,SourceCodePro-Regular-8badfe75.ttf.woff2,SourceCodePro-Semibold-aa29a496.ttf.woff2".split(",").map(f=>`<link rel="preload" as="font" type="font/woff2"href="../../static.files/${f}">`).join(""))</script><link rel="stylesheet" href="../../static.files/normalize-9960930a.css"><link rel="stylesheet" href="../../static.files/rustdoc-ca0dd0c4.css"><meta name="rustdoc-vars" data-root-path="../../" data-static-root-path="../../static.files/" data-current-crate="hippo_core" data-themes="" data-resource-suffix="" data-rustdoc-version="1.93.0 (254b59607 2026-01-19)" data-channel="1.93.0" data-search-js="search-9e2438ea.js" data-stringdex-js="stringdex-a3946164.js" data-settings-js="settings-c38705f0.js" ><script src="../../static.files/storage-e2aeef58.js"></script><script defer src="../sidebar-items.js"></script><script defer src="../../static.files/main-a410ff4d.js"></script><noscript><link rel="stylesheet" href="../../static.files/noscript-263c88ec.css"></noscript><link rel="alternate icon" type="image/png" href="../../static.files/favicon-32x32-eab170b8.png"><link rel="icon" type="image/svg+xml" href="../../static.files/favicon-044be391.svg"></head><body class="rustdoc mod"><!--[if lte IE 11]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><rustdoc-topbar><h2><a href="#">Module ollama</a></h2></rustdoc-topbar><nav class="sidebar"><div class="sidebar-crate"><h2><a href="../../hippo_core/index.html">hippo_<wbr>core</a><span class="version">1.2.0</span></h2></div><div class="sidebar-elems"><section id="rustdoc-toc"><h2 class="location"><a href="#">Module ollama</a></h2><h3><a href="#structs">Module Items</a></h3><ul class="block"><li><a href="#structs" title="Structs">Structs</a></li><li><a href="#constants" title="Constants">Constants</a></li></ul></section><div id="rustdoc-modnav"><h2 class="in-crate"><a href="../index.html">In crate hippo_<wbr>core</a></h2></div></div></nav><div class="sidebar-resizer" title="Drag to resize sidebar"></div><main><div class="width-limiter"><section id="main-content" class="content"><div class="main-heading"><div class="rustdoc-breadcrumbs"><a href="../index.html">hippo_core</a></div><h1>Module <span>ollama</span>&nbsp;<button id="copy-path" title="Copy item path to clipboard">Copy item path</button></h1><rustdoc-toolbar></rustdoc-toolbar><span class="sub-heading"><a class="src" href="../../src/hippo_core/ollama/mod.rs.html#1-1227">Source</a> </span></div><details class="toggle top-doc" open><summary class="hideme"><span>Expand description</span></summary><div class="docblock"><p>Ollama integration for local AI capabilities</p>
<p>Supports local embeddings, text generation, and RAG pipelines.
Includes support for high-quality models like Gemma2, Llama3.2, and Qwen2.5.</p>
<p>Performance optimizations:</p>
<ul>
<li>Model availability caching (5-minute TTL)</li>
<li>Embedding response caching (1-hour TTL, 1000 entries)</li>
<li>Connection pooling with keep-alive</li>
</ul>
</div></details><h2 id="structs" class="section-header">Structs<a href="#structs" class="anchor">ยง</a></h2><dl class="item-table"><dt><a class="struct" href="struct.ChatMessage.html" title="struct hippo_core::ollama::ChatMessage">Chat<wbr>Message</a></dt><dd>Chat message for conversation</dd><dt><a class="struct" href="struct.LocalAnalysis.html" title="struct hippo_core::ollama::LocalAnalysis">Local<wbr>Analysis</a></dt><dd>Result of an AI analysis</dd><dt><a class="struct" href="struct.ModelDetails.html" title="struct hippo_core::ollama::ModelDetails">Model<wbr>Details</a></dt><dt><a class="struct" href="struct.OllamaClient.html" title="struct hippo_core::ollama::OllamaClient">Ollama<wbr>Client</a></dt><dd>Ollama client for local AI operations with caching</dd><dt><a class="struct" href="struct.OllamaConfig.html" title="struct hippo_core::ollama::OllamaConfig">Ollama<wbr>Config</a></dt><dd>Configuration for Ollama client</dd><dt><a class="struct" href="struct.OllamaModel.html" title="struct hippo_core::ollama::OllamaModel">Ollama<wbr>Model</a></dt><dd>Information about an Ollama model</dd><dt><a class="struct" href="struct.PullProgress.html" title="struct hippo_core::ollama::PullProgress">Pull<wbr>Progress</a></dt><dd>Pull progress response</dd><dt><a class="struct" href="struct.RagContext.html" title="struct hippo_core::ollama::RagContext">RagContext</a></dt><dd>RAG context for generation</dd><dt><a class="struct" href="struct.RagDocument.html" title="struct hippo_core::ollama::RagDocument">RagDocument</a></dt><dt><a class="struct" href="struct.RecommendedModels.html" title="struct hippo_core::ollama::RecommendedModels">Recommended<wbr>Models</a></dt><dd>Recommended models for different use cases</dd><dt><a class="struct" href="struct.StreamChunk.html" title="struct hippo_core::ollama::StreamChunk">Stream<wbr>Chunk</a></dt><dd>Stream chunk from Ollama</dd></dl><h2 id="constants" class="section-header">Constants<a href="#constants" class="anchor">ยง</a></h2><dl class="item-table"><dt><a class="constant" href="constant.DEFAULT_EMBEDDING_MODEL.html" title="constant hippo_core::ollama::DEFAULT_EMBEDDING_MODEL">DEFAULT_<wbr>EMBEDDING_<wbr>MODEL</a></dt><dd>Default embedding model</dd><dt><a class="constant" href="constant.DEFAULT_GENERATION_MODEL.html" title="constant hippo_core::ollama::DEFAULT_GENERATION_MODEL">DEFAULT_<wbr>GENERATION_<wbr>MODEL</a></dt><dd>Default generation model - Gemma2 2B offers best quality/speed ratio (~1.6GB)</dd><dt><a class="constant" href="constant.DEFAULT_OLLAMA_URL.html" title="constant hippo_core::ollama::DEFAULT_OLLAMA_URL">DEFAULT_<wbr>OLLAMA_<wbr>URL</a></dt><dd>Default Ollama API endpoint</dd><dt><a class="constant" href="constant.DEFAULT_TAGGING_MODEL.html" title="constant hippo_core::ollama::DEFAULT_TAGGING_MODEL">DEFAULT_<wbr>TAGGING_<wbr>MODEL</a></dt><dd>Ultra-fast tagging model - Qwen2 0.5B for instant auto-tagging (~352MB)
Falls back to llama3.2:1b if not available</dd><dt><a class="constant" href="constant.NOMIC_EMBED_DIM.html" title="constant hippo_core::ollama::NOMIC_EMBED_DIM">NOMIC_<wbr>EMBED_<wbr>DIM</a></dt><dd>Embedding dimension for nomic-embed-text</dd></dl></section></div></main></body></html>